wandb_version: 1

Attention:
  desc: null
  value:
    cell_type: LSTM
    embedding_dim: 128
    encoder_type: resnet
    hidden_dim: 128
    layer_num: 1
    seq_hidden_dim: 256
    src_dim: 512
SATRN:
  desc: null
  value:
    decoder:
      filter_dim: 512
      head_num: 8
      hidden_dim: 128
      layer_num: 3
      src_dim: 300
    encoder:
      filter_dim: 600
      head_num: 8
      hidden_dim: 300
      layer_num: 6
_wandb:
  desc: null
  value:
    cli_version: 0.10.30
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.7.7
    t:
      1:
      - 1
      2:
      - 1
      4: 3.7.7
      5: 0.10.30
      8:
      - 5
augmentation:
  desc: null
  value: baseline
batch_size:
  desc: null
  value: 128
checkpoint:
  desc: null
  value: ./log/attention_50/checkpoints/0031.pth
data:
  desc: null
  value:
    crop: true
    dataset_proportions:
    - 1.0
    random_split: true
    rgb: 1
    test:
    - ''
    test_proportions: 0.2
    token_paths:
    - /opt/ml/input/data/train_dataset/tokens.txt
    train:
    - /opt/ml/input/data/train_dataset/gt.txt
dropout_rate:
  desc: null
  value: 0.1
input_size:
  desc: null
  value:
    height: 128
    width: 128
max_grad_norm:
  desc: null
  value: 2.0
network:
  desc: null
  value: Attention
num_epochs:
  desc: null
  value: 70
num_workers:
  desc: null
  value: 8
optimizer:
  desc: null
  value:
    lr: 5e-4
    optimizer: Adam
    type: cycle
    weight_decay: 1e-4
prefix:
  desc: null
  value: ./log/attention_50
print_epochs:
  desc: null
  value: 1
ratio_cycle:
  desc: null
  value: 20
seed:
  desc: null
  value: 914
teacher_forcing_ratio:
  desc: null
  value:
  - 0.7
  - 0.6
  - 0.5
  - 0.5
